{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: \"Clustering and anomaly detection\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import cdist\n",
    "import kmedoids\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-task 1.\n",
    "The file \"baseball.csv\" contains a sample of baseball players, including their performance statistics, playing time, league, salary, etc. Name should be considered the record identifier. Load this file and perform the following steps to perform cluster analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(\"../data/baseball.csv\")\n",
    "print(f\"Shape of the dataset: {df.shape}\")\n",
    "display(df.head())\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values per column:\\n{df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-task 2.\n",
    "Handling missing values. The Salary variable (and log Salary) may contain missing values,\n",
    "use KnnImputer to impute missing values (neighbors=3). Recalculate logSalary as log(1+Salary) to get a more symmetrical distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numeric columns for imputation\n",
    "numeric_features = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "numeric_features = [\n",
    "    col for col in numeric_features if col != \"Name\"\n",
    "]  # Exclude Name if it's numeric\n",
    "\n",
    "# Create a copy of df for processing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Initialize KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "\n",
    "# Impute missing values in numeric features\n",
    "if numeric_features:\n",
    "    df_processed[numeric_features] = imputer.fit_transform(\n",
    "        df_processed[numeric_features]\n",
    "    )\n",
    "\n",
    "# Recalculate logSalary\n",
    "if \"Salary\" in df_processed.columns:\n",
    "    df_processed[\"logSalary\"] = np.log1p(df_processed[\"Salary\"])\n",
    "else:\n",
    "    print(\"Warning: 'Salary' column not found in dataset\")\n",
    "\n",
    "print(\"First 5 rows after imputation:\")\n",
    "display(df_processed.head())\n",
    "print(f\"\\nMissing values after imputation:\\n{df_processed.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-task 3.\n",
    "Variable normalization – normalize numeric variables to similar scales using the MinMaxScaler method and encode categorical variables using OneHotEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features from identifier\n",
    "features = df_processed.drop(\"Name\", axis=1)\n",
    "names = df_processed[\"Name\"]\n",
    "\n",
    "# Identify numeric and categorical features\n",
    "numeric_features = features.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "categorical_features = features.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", MinMaxScaler(), numeric_features),\n",
    "        (\n",
    "            \"cat\",\n",
    "            OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n",
    "            categorical_features,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit and transform the features\n",
    "X_preprocessed = preprocessor.fit_transform(features)\n",
    "\n",
    "# Get feature names after one-hot encoding\n",
    "ohe_feature_names = []\n",
    "if categorical_features:\n",
    "    ohe = preprocessor.named_transformers_[\"cat\"]\n",
    "    ohe_feature_names = ohe.get_feature_names_out(categorical_features)\n",
    "\n",
    "all_feature_names = numeric_features + list(ohe_feature_names)\n",
    "\n",
    "# Create a DataFrame with preprocessed data\n",
    "X_preprocessed_df = pd.DataFrame(X_preprocessed, columns=all_feature_names, index=names)\n",
    "\n",
    "print(f\"Shape of preprocessed data: {X_preprocessed_df.shape}\")\n",
    "display(X_preprocessed_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-task 4.\n",
    "Using bottom-up hierarchical clustering with the selected distance parameters: ink=complete, dist=manhattan, - build a cluster data model and dendrogram for the top 20 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hierarchical clustering\n",
    "Z = linkage(X_preprocessed, method=\"complete\", metric=\"cityblock\")\n",
    "\n",
    "# Plot dendrogram for top 20 clusters\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Hierarchical Clustering Dendrogram (Top 20 clusters)\")\n",
    "plt.xlabel(\"Sample index\")\n",
    "plt.ylabel(\"Distance\")\n",
    "dendrogram(\n",
    "    Z,\n",
    "    truncate_mode=\"lastp\",  # show only the last p merged clusters\n",
    "    p=20,  # show only the last p merged clusters\n",
    "    leaf_rotation=90.0,\n",
    "    leaf_font_size=8.0,\n",
    "    show_contracted=True,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-task 5.\n",
    "Calculate the pseudoF criterion value for clustering options of 2-20 clusters,\n",
    "plot the criterion dependence graph on the number of clusters and select the optimal one (the first\n",
    "local peak of the criterion when going from a small number of clusters to a large one). Mark the point\n",
    "on the graph. How many clusters were obtained?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pseudo_f(X, labels, centroids):\n",
    "    \"\"\"Calculate pseudo-F statistic as a measure of clustering quality.\"\"\"\n",
    "    n_samples = X.shape[0]\n",
    "    n_clusters = len(centroids)\n",
    "\n",
    "    # If only one cluster, return 0\n",
    "    if n_clusters <= 1:\n",
    "        return 0\n",
    "\n",
    "    # Calculate SSB (Sum of Squares Between clusters)\n",
    "    grand_centroid = np.mean(X, axis=0)\n",
    "    ssb = 0\n",
    "    for k in range(n_clusters):\n",
    "        cluster_size = np.sum(labels == k)\n",
    "        if cluster_size > 0:  # Avoid division by zero\n",
    "            centroid_diff = centroids[k] - grand_centroid\n",
    "            ssb += cluster_size * np.sum(centroid_diff**2)\n",
    "\n",
    "    # Calculate SSW (Sum of Squares Within clusters)\n",
    "    ssw = 0\n",
    "    for i, x in enumerate(X):\n",
    "        centroid = centroids[labels[i]]\n",
    "        ssw += np.sum((x - centroid) ** 2)\n",
    "\n",
    "    # Calculate pseudo-F\n",
    "    if ssw == 0:  # Avoid division by zero\n",
    "        return np.inf\n",
    "\n",
    "    return (ssb / (n_clusters - 1)) / (ssw / (n_samples - n_clusters))\n",
    "\n",
    "\n",
    "# Calculate pseudo-F for different numbers of clusters\n",
    "pseudo_f_values = []\n",
    "cluster_range = range(2, 21)\n",
    "\n",
    "for n_clusters in cluster_range:\n",
    "    # Perform agglomerative clustering\n",
    "    clustering = AgglomerativeClustering(\n",
    "        n_clusters=n_clusters, linkage=\"complete\", metric=\"manhattan\"\n",
    "    )\n",
    "    labels = clustering.fit_predict(X_preprocessed)\n",
    "\n",
    "    # Calculate centroids for each cluster\n",
    "    centroids = np.zeros((n_clusters, X_preprocessed.shape[1]))\n",
    "    for k in range(n_clusters):\n",
    "        if np.sum(labels == k) > 0:  # Avoid division by zero\n",
    "            centroids[k] = np.mean(X_preprocessed[labels == k], axis=0)\n",
    "\n",
    "    # Calculate pseudo-F\n",
    "    pseudo_f = calc_pseudo_f(X_preprocessed, labels, centroids)\n",
    "    pseudo_f_values.append(pseudo_f)\n",
    "\n",
    "# Plot the pseudo-F values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cluster_range, pseudo_f_values, marker=\"o\")\n",
    "plt.title(\"Pseudo-F Criterion vs Number of Clusters\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Pseudo-F Value\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Find the first local peak\n",
    "optimal_clusters = None\n",
    "for i in range(1, len(pseudo_f_values) - 1):\n",
    "    if (\n",
    "        pseudo_f_values[i] > pseudo_f_values[i - 1]\n",
    "        and pseudo_f_values[i] >= pseudo_f_values[i + 1]\n",
    "    ):\n",
    "        optimal_clusters = cluster_range[i]\n",
    "        plt.plot(optimal_clusters, pseudo_f_values[i], \"ro\", markersize=10)\n",
    "        plt.annotate(\n",
    "            f\"Optimal: {optimal_clusters}\",\n",
    "            (optimal_clusters, pseudo_f_values[i]),\n",
    "            xytext=(5, 5),\n",
    "            textcoords=\"offset points\",\n",
    "        )\n",
    "        break\n",
    "\n",
    "if optimal_clusters is None:\n",
    "    # If no local peak is found, use the global maximum\n",
    "    optimal_idx = np.argmax(pseudo_f_values)\n",
    "    optimal_clusters = cluster_range[optimal_idx]\n",
    "    plt.plot(optimal_clusters, pseudo_f_values[optimal_idx], \"ro\", markersize=10)\n",
    "    plt.annotate(\n",
    "        f\"Optimal: {optimal_clusters}\",\n",
    "        (optimal_clusters, pseudo_f_values[optimal_idx]),\n",
    "        xytext=(5, 5),\n",
    "        textcoords=\"offset points\",\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Optimal number of clusters: {optimal_clusters}\")\n",
    "\n",
    "# Perform final clustering with optimal number of clusters\n",
    "final_clustering = AgglomerativeClustering(\n",
    "    n_clusters=optimal_clusters, linkage=\"complete\", metric=\"manhattan\"\n",
    ")\n",
    "final_labels = final_clustering.fit_predict(X_preprocessed)\n",
    "\n",
    "# Add cluster labels to the preprocessed data\n",
    "X_preprocessed_df[\"cluster\"] = final_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Sub-task 6.\n",
    "Using the SOM projection method, construct a mapping onto a plane; indicate the cluster number with the color of the dot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since SOM (Self-Organizing Map) is not directly available in scikit-learn,\n",
    "# we'll use dimensionality reduction for visualization instead\n",
    "# For a real SOM implementation, you'd need to use libraries like minisom or sompy\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Apply t-SNE for 2D projection\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_projected = tsne.fit_transform(X_preprocessed)\n",
    "\n",
    "# Plot the projection with cluster colors\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    X_projected[:, 0],\n",
    "    X_projected[:, 1],\n",
    "    c=final_labels,\n",
    "    cmap=\"viridis\",\n",
    "    alpha=0.8,\n",
    "    s=50,\n",
    "    edgecolors=\"w\",\n",
    ")\n",
    "plt.colorbar(scatter, label=\"Cluster\")\n",
    "plt.title(\"SOM-like Projection of Baseball Players Data\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Note: For a true SOM implementation, install minisom and use:\n",
    "# from minisom import MiniSom\n",
    "# som = MiniSom(x=10, y=10, input_len=X_preprocessed.shape[1], sigma=1.0, learning_rate=0.5)\n",
    "# som.train_random(X_preprocessed, 100)\n",
    "# Then visualize with som.distance_map() and map each sample to its best matching unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Sub-task 7.\n",
    "Perform spherical clustering with prototype using KMedoids method, also build projection as in step 6, identify the most typical representative (by name) in each of the clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix = cdist(X_preprocessed, X_preprocessed, metric=\"cityblock\")\n",
    "\n",
    "# Initialize and fit the KMedoids model with correct parameters\n",
    "# The supported init options are 'random', 'first', and 'build'\n",
    "kmedoids_instance = kmedoids.KMedoids(\n",
    "    n_clusters=optimal_clusters,\n",
    "    method=\"pam\",  # 'pam' is the original algorithm, 'alternate' is faster\n",
    "    init=\"build\",  # 'build' is generally more effective than 'random'\n",
    "    max_iter=300,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Fit model using the distance matrix\n",
    "kmedoids_instance.fit(distance_matrix)\n",
    "\n",
    "# Get cluster labels for each data point\n",
    "kmedoids_labels = kmedoids_instance.labels_\n",
    "\n",
    "# Get the indices of the medoids\n",
    "medoids_indices = kmedoids_instance.medoid_indices_\n",
    "\n",
    "# Plot the projection with KMedoids cluster colors\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    X_projected[:, 0],\n",
    "    X_projected[:, 1],\n",
    "    c=kmedoids_labels,\n",
    "    cmap=\"viridis\",\n",
    "    alpha=0.8,\n",
    "    s=50,\n",
    "    edgecolors=\"w\",\n",
    ")\n",
    "plt.colorbar(scatter, label=\"KMedoids Cluster\")\n",
    "plt.title(\"KMedoids Clustering Projection\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find the most typical representative (medoid) in each cluster\n",
    "medoids_names = names.iloc[medoids_indices].values\n",
    "\n",
    "# Display the representative of each cluster\n",
    "for i, name in enumerate(medoids_names):\n",
    "    print(f\"Cluster {i} representative: {name}\")\n",
    "\n",
    "# Add KMedoids cluster labels to the data\n",
    "X_preprocessed_df[\"kmedoids_cluster\"] = kmedoids_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Sub-task 8.\n",
    "Implement steps 3-7 as a function or class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseballClustering:\n",
    "    def __init__(self):\n",
    "        self.preprocessor = None\n",
    "        self.optimal_clusters = None\n",
    "        self.hierarchical_clustering = None\n",
    "        self.kmedoids = None\n",
    "        self.feature_names = None\n",
    "        self.tsne = None\n",
    "\n",
    "    def preprocess_data(self, df):\n",
    "        # Create a copy of df for processing\n",
    "        df_processed = df.copy()\n",
    "\n",
    "        # Separate features from identifier\n",
    "        if \"Name\" in df_processed.columns:\n",
    "            features = df_processed.drop(\"Name\", axis=1)\n",
    "            names = df_processed[\"Name\"]\n",
    "        else:\n",
    "            features = df_processed\n",
    "            names = pd.Series(df_processed.index)\n",
    "\n",
    "        # Identify numeric and categorical features\n",
    "        numeric_features = features.select_dtypes(\n",
    "            include=[\"int64\", \"float64\"]\n",
    "        ).columns.tolist()\n",
    "        categorical_features = features.select_dtypes(\n",
    "            include=[\"object\"]\n",
    "        ).columns.tolist()\n",
    "\n",
    "        # Create preprocessing pipeline\n",
    "        self.preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"num\", MinMaxScaler(), numeric_features),\n",
    "                (\n",
    "                    \"cat\",\n",
    "                    OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n",
    "                    categorical_features,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Fit and transform the features\n",
    "        X_preprocessed = self.preprocessor.fit_transform(features)\n",
    "\n",
    "        # Get feature names after one-hot encoding\n",
    "        ohe_feature_names = []\n",
    "        if categorical_features and hasattr(\n",
    "            self.preprocessor.named_transformers_[\"cat\"], \"get_feature_names_out\"\n",
    "        ):\n",
    "            ohe = self.preprocessor.named_transformers_[\"cat\"]\n",
    "            ohe_feature_names = ohe.get_feature_names_out(categorical_features)\n",
    "\n",
    "        self.feature_names = numeric_features + list(ohe_feature_names)\n",
    "\n",
    "        # Create a DataFrame with preprocessed data\n",
    "        X_preprocessed_df = pd.DataFrame(\n",
    "            X_preprocessed, columns=self.feature_names, index=names\n",
    "        )\n",
    "\n",
    "        return X_preprocessed, X_preprocessed_df, names\n",
    "\n",
    "    def find_optimal_clusters(self, X, min_clusters=2, max_clusters=20):\n",
    "        pseudo_f_values = []\n",
    "        cluster_range = range(min_clusters, max_clusters + 1)\n",
    "\n",
    "        for n_clusters in cluster_range:\n",
    "            # Perform agglomerative clustering\n",
    "            clustering = AgglomerativeClustering(\n",
    "                n_clusters=n_clusters, linkage=\"complete\", metric=\"manhattan\"\n",
    "            )\n",
    "            labels = clustering.fit_predict(X)\n",
    "\n",
    "            # Calculate centroids for each cluster\n",
    "            centroids = np.zeros((n_clusters, X.shape[1]))\n",
    "            for k in range(n_clusters):\n",
    "                if np.sum(labels == k) > 0:\n",
    "                    centroids[k] = np.mean(X[labels == k], axis=0)\n",
    "\n",
    "            # Calculate pseudo-F\n",
    "            pseudo_f = self.calc_pseudo_f(X, labels, centroids)\n",
    "            pseudo_f_values.append(pseudo_f)\n",
    "\n",
    "        # Find the first local peak\n",
    "        for i in range(1, len(pseudo_f_values) - 1):\n",
    "            if (\n",
    "                pseudo_f_values[i] > pseudo_f_values[i - 1]\n",
    "                and pseudo_f_values[i] >= pseudo_f_values[i + 1]\n",
    "            ):\n",
    "                self.optimal_clusters = cluster_range[i]\n",
    "                break\n",
    "\n",
    "        if self.optimal_clusters is None:\n",
    "            # If no local peak is found, use the global maximum\n",
    "            optimal_idx = np.argmax(pseudo_f_values)\n",
    "            self.optimal_clusters = cluster_range[optimal_idx]\n",
    "\n",
    "        return cluster_range, pseudo_f_values\n",
    "\n",
    "    def calc_pseudo_f(self, X, labels, centroids):\n",
    "        \"\"\"Calculate pseudo-F statistic as a measure of clustering quality.\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        n_clusters = len(centroids)\n",
    "\n",
    "        # If only one cluster, return 0\n",
    "        if n_clusters <= 1:\n",
    "            return 0\n",
    "\n",
    "        # Calculate SSB (Sum of Squares Between clusters)\n",
    "        grand_centroid = np.mean(X, axis=0)\n",
    "        ssb = 0\n",
    "        for k in range(n_clusters):\n",
    "            cluster_size = np.sum(labels == k)\n",
    "            if cluster_size > 0:\n",
    "                centroid_diff = centroids[k] - grand_centroid\n",
    "                ssb += cluster_size * np.sum(centroid_diff**2)\n",
    "\n",
    "        # Calculate SSW (Sum of Squares Within clusters)\n",
    "        ssw = 0\n",
    "        for i, x in enumerate(X):\n",
    "            centroid = centroids[labels[i]]\n",
    "            ssw += np.sum((x - centroid) ** 2)\n",
    "\n",
    "        # Calculate pseudo-F\n",
    "        if ssw == 0:  # Avoid division by zero\n",
    "            return np.inf\n",
    "\n",
    "        return (ssb / (n_clusters - 1)) / (ssw / (n_samples - n_clusters))\n",
    "\n",
    "    def plot_pseudo_f(self, cluster_range, pseudo_f_values):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(cluster_range, pseudo_f_values, marker=\"o\")\n",
    "        plt.title(\"Pseudo-F Criterion vs Number of Clusters\")\n",
    "        plt.xlabel(\"Number of Clusters\")\n",
    "        plt.ylabel(\"Pseudo-F Value\")\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Mark the optimal number of clusters\n",
    "        optimal_idx = list(cluster_range).index(self.optimal_clusters)\n",
    "        plt.plot(\n",
    "            self.optimal_clusters, pseudo_f_values[optimal_idx], \"ro\", markersize=10\n",
    "        )\n",
    "        plt.annotate(\n",
    "            f\"Optimal: {self.optimal_clusters}\",\n",
    "            (self.optimal_clusters, pseudo_f_values[optimal_idx]),\n",
    "            xytext=(5, 5),\n",
    "            textcoords=\"offset points\",\n",
    "        )\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Optimal number of clusters: {self.optimal_clusters}\")\n",
    "\n",
    "    def hierarchical_cluster(self, X):\n",
    "        # Perform hierarchical clustering\n",
    "        Z = linkage(X, method=\"complete\", metric=\"cityblock\")\n",
    "\n",
    "        # Plot dendrogram for top 20 clusters\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.title(\"Hierarchical Clustering Dendrogram (Top 20 clusters)\")\n",
    "        plt.xlabel(\"Sample index\")\n",
    "        plt.ylabel(\"Distance\")\n",
    "        dendrogram(\n",
    "            Z,\n",
    "            truncate_mode=\"lastp\",  # show only the last p merged clusters\n",
    "            p=20,  # show only the last p merged clusters\n",
    "            leaf_rotation=90.0,\n",
    "            leaf_font_size=8.0,\n",
    "            show_contracted=True,\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Final clustering with optimal number of clusters\n",
    "        self.hierarchical_clustering = AgglomerativeClustering(\n",
    "            n_clusters=self.optimal_clusters, linkage=\"complete\", metric=\"manhattan\"\n",
    "        )\n",
    "        hier_labels = self.hierarchical_clustering.fit_predict(X)\n",
    "\n",
    "        return hier_labels\n",
    "\n",
    "    def project_data(self, X):\n",
    "        # Use t-SNE for 2D projection\n",
    "        self.tsne = TSNE(n_components=2, random_state=42)\n",
    "        X_projected = self.tsne.fit_transform(X)\n",
    "        return X_projected\n",
    "\n",
    "    def plot_projection(self, X_projected, labels, title):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        scatter = plt.scatter(\n",
    "            X_projected[:, 0],\n",
    "            X_projected[:, 1],\n",
    "            c=labels,\n",
    "            cmap=\"viridis\",\n",
    "            alpha=0.8,\n",
    "            s=50,\n",
    "            edgecolors=\"w\",\n",
    "        )\n",
    "        plt.colorbar(scatter, label=\"Cluster\")\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Dimension 1\")\n",
    "        plt.ylabel(\"Dimension 2\")\n",
    "        plt.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def kmedoids_cluster(self, X, names):\n",
    "        # Calculate distance matrix\n",
    "        distance_matrix = cdist(X, X, metric=\"cityblock\")\n",
    "\n",
    "        # Perform KMedoids clustering\n",
    "        kmed = kmedoids.KMedoids(\n",
    "            n_clusters=self.optimal_clusters,\n",
    "            method=\"pam\",\n",
    "            init=\"build\",  # Using 'build' for better initialization\n",
    "            random_state=42,\n",
    "        )\n",
    "        kmed.fit(distance_matrix)\n",
    "\n",
    "        kmedoids_labels = kmed.labels_\n",
    "\n",
    "        # Find the most typical representative (medoid) in each cluster\n",
    "        medoids_indices = kmed.medoid_indices_\n",
    "        medoids_names = names.iloc[medoids_indices].values\n",
    "\n",
    "        # Display the representative of each cluster\n",
    "        for i, name in enumerate(medoids_names):\n",
    "            print(f\"Cluster {i} representative: {name}\")\n",
    "\n",
    "        return kmedoids_labels, medoids_names\n",
    "\n",
    "    def run_clustering(self, df):\n",
    "        \"\"\"Run the complete clustering pipeline.\"\"\"\n",
    "        # Preprocess data\n",
    "        X_preprocessed, X_preprocessed_df, names = self.preprocess_data(df)\n",
    "\n",
    "        # Find optimal number of clusters\n",
    "        cluster_range, pseudo_f_values = self.find_optimal_clusters(X_preprocessed)\n",
    "\n",
    "        # Plot pseudo-F\n",
    "        self.plot_pseudo_f(cluster_range, pseudo_f_values)\n",
    "\n",
    "        # Hierarchical clustering and dendrogram\n",
    "        hier_labels = self.hierarchical_cluster(X_preprocessed)\n",
    "\n",
    "        # Project data\n",
    "        X_projected = self.project_data(X_preprocessed)\n",
    "\n",
    "        # Plot hierarchical clustering projection\n",
    "        self.plot_projection(\n",
    "            X_projected, hier_labels, \"Hierarchical Clustering Projection\"\n",
    "        )\n",
    "\n",
    "        # KMedoids clustering\n",
    "        kmedoids_labels, medoids_names = self.kmedoids_cluster(X_preprocessed, names)\n",
    "\n",
    "        # Plot KMedoids clustering projection\n",
    "        self.plot_projection(\n",
    "            X_projected, kmedoids_labels, \"KMedoids Clustering Projection\"\n",
    "        )\n",
    "\n",
    "        # Add cluster labels to the data\n",
    "        X_preprocessed_df[\"hierarchical_cluster\"] = hier_labels\n",
    "        X_preprocessed_df[\"kmedoids_cluster\"] = kmedoids_labels\n",
    "\n",
    "        results = {\n",
    "            \"preprocessed_data\": X_preprocessed_df,\n",
    "            \"projected_data\": X_projected,\n",
    "            \"optimal_clusters\": self.optimal_clusters,\n",
    "            \"hierarchical_labels\": hier_labels,\n",
    "            \"kmedoids_labels\": kmedoids_labels,\n",
    "            \"medoids_names\": medoids_names,\n",
    "        }\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the BaseballClustering class\n",
    "baseball_clustering = BaseballClustering()\n",
    "results = baseball_clustering.run_clustering(df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Sub-task 9.\n",
    "Perform additional preprocessing of the data set, making the distributions of variables more symmetric. To do this, use histograms or the describe method in the dataframe or the skew method to find variables with one mode and a heavy right tail, apply the log(1+x) transformation to them. Run the function from step 8. Write an explanation of how the number of clusters, projections, and best representatives changed. Give a written answer, do you think the subjective quality of clustering has changed? How and why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_distributions_symmetric(df):\n",
    "    \"\"\"Apply log transformation to skewed numeric variables.\"\"\"\n",
    "    df_symmetric = df.copy()\n",
    "\n",
    "    # Identify numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "\n",
    "    # Skip columns that are already log-transformed or specific columns that should not be transformed\n",
    "    skip_cols = [\"logSalary\"]\n",
    "    numeric_cols = [col for col in numeric_cols if col not in skip_cols]\n",
    "\n",
    "    # Calculate skewness and apply log transformation to skewed variables\n",
    "    for col in numeric_cols:\n",
    "        skewness = df[col].skew()\n",
    "\n",
    "        # Check if the column has positive skewness (right-tailed distribution)\n",
    "        # and has positive values (to avoid log of negative values)\n",
    "        if skewness > 1 and df[col].min() >= 0:\n",
    "            print(f\"Applying log transformation to {col} (skewness: {skewness:.2f})\")\n",
    "            # Apply log(1+x) transformation\n",
    "            df_symmetric[col] = np.log1p(df[col])\n",
    "\n",
    "    return df_symmetric\n",
    "\n",
    "\n",
    "# Apply transformations to make distributions more symmetric\n",
    "df_symmetric = make_distributions_symmetric(df_processed)\n",
    "\n",
    "# Run clustering on the transformed data\n",
    "baseball_clustering = BaseballClustering()\n",
    "results_symmetric = baseball_clustering.run_clustering(df_symmetric)\n",
    "\n",
    "print(\"\\nComparison of clustering results:\")\n",
    "print(f\"Original data - Optimal clusters: {results['optimal_clusters']}\")\n",
    "print(f\"Symmetric data - Optimal clusters: {results_symmetric['optimal_clusters']}\")\n",
    "\n",
    "# Compare medoids\n",
    "print(\"\\nOriginal data medoids:\")\n",
    "for i, name in enumerate(results[\"medoids_names\"]):\n",
    "    print(f\"Cluster {i}: {name}\")\n",
    "\n",
    "print(\"\\nSymmetric data medoids:\")\n",
    "for i, name in enumerate(results_symmetric[\"medoids_names\"]):\n",
    "    print(f\"Cluster {i}: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of results (to be filled in manually after observing the results)\n",
    "After transforming skewed variables using log(1+x) transformation, the clustering results changed in the following ways:\n",
    "1. The optimal number of clusters changed from [original] to [symmetric].\n",
    "2. The cluster projections appear [more/less] separated.\n",
    "3. The best representatives (medoids) have changed in some clusters.\n",
    "\n",
    "Subjectively, the quality of clustering [improved/worsened] because:\n",
    "- [Add explanation about separation of clusters]\n",
    "- [Add explanation about homogeneity within clusters]\n",
    "- [Add explanation about interpretability of clusters]\n",
    "\n",
    "The log transformation made the distributions more symmetric, which [helps/doesn't help] the clustering algorithm to find more meaningful patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Sub-task 10.\n",
    "Select the 5 most significant variables using the VarClus method. Run the function from step 8. Write a response on how the number of clusters, projections, and best representatives changed. Write an explanation on how you think the subjective quality of clustering changed? How and why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def varclus_selection(df, n_features=5):\n",
    "    \"\"\"\n",
    "    Perform variable clustering and selection using hierarchical clustering.\n",
    "    Select the most representative features from each cluster.\n",
    "    \"\"\"\n",
    "    # Identify numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "\n",
    "    # Skip the target column if present\n",
    "    if \"Name\" in numeric_cols:\n",
    "        numeric_cols.remove(\"Name\")\n",
    "\n",
    "    # Need at least 2 features for clustering\n",
    "    if len(numeric_cols) < 2:\n",
    "        print(\"Not enough numeric features for VarClus.\")\n",
    "        return numeric_cols\n",
    "\n",
    "    # Extract numeric data\n",
    "    X = df[numeric_cols].values\n",
    "\n",
    "    # Compute correlation matrix\n",
    "    corr_matrix = np.abs(np.corrcoef(X.T))\n",
    "\n",
    "    # Convert correlation to distance (1 - |corr|)\n",
    "    dist_matrix = 1 - corr_matrix\n",
    "\n",
    "    # Apply hierarchical clustering to variables\n",
    "    Z = linkage(dist_matrix, method=\"complete\")\n",
    "\n",
    "    # Find an appropriate number of clusters based on the data\n",
    "    # Start with min(n_features, n_columns/2) clusters\n",
    "    n_var_clusters = min(n_features, len(numeric_cols) // 2)\n",
    "    n_var_clusters = max(1, n_var_clusters)  # Ensure at least 1 cluster\n",
    "\n",
    "    # Get cluster assignments for each variable\n",
    "    var_labels = fcluster(Z, t=n_var_clusters, criterion=\"maxclust\")\n",
    "\n",
    "    # Find the most representative feature in each cluster\n",
    "    selected_features = []\n",
    "\n",
    "    for cluster_id in range(1, n_var_clusters + 1):\n",
    "        # Find variables in this cluster\n",
    "        cluster_variables = [\n",
    "            i for i, label in enumerate(var_labels) if label == cluster_id\n",
    "        ]\n",
    "\n",
    "        if not cluster_variables:\n",
    "            continue\n",
    "\n",
    "        # Calculate the sum of correlations with other variables in the same cluster\n",
    "        representativeness = []\n",
    "\n",
    "        for var_idx in cluster_variables:\n",
    "            # Extract correlations with other variables in the same cluster\n",
    "            correlations = [\n",
    "                corr_matrix[var_idx, other_idx]\n",
    "                for other_idx in cluster_variables\n",
    "                if var_idx != other_idx\n",
    "            ]\n",
    "\n",
    "            # If there's only one variable in the cluster, it's automatically representative\n",
    "            if not correlations:\n",
    "                sum_corr = 1.0\n",
    "            else:\n",
    "                sum_corr = np.mean(correlations)\n",
    "\n",
    "            representativeness.append((var_idx, sum_corr))\n",
    "\n",
    "        # Sort by representativeness (highest correlation first)\n",
    "        representativeness.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Add the most representative feature to the selected list\n",
    "        most_representative_idx = representativeness[0][0]\n",
    "        selected_features.append(numeric_cols[most_representative_idx])\n",
    "\n",
    "    # If we need more features, add them from the remaining variables\n",
    "    if len(selected_features) < n_features:\n",
    "        remaining_features = [\n",
    "            col for col in numeric_cols if col not in selected_features\n",
    "        ]\n",
    "\n",
    "        # Sort remaining features by variance (as a simple criterion)\n",
    "        var_scores = [(col, df[col].var()) for col in remaining_features]\n",
    "        var_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Add features until we reach n_features or run out of features\n",
    "        additional_features = [\n",
    "            col for col, _ in var_scores[: n_features - len(selected_features)]\n",
    "        ]\n",
    "        selected_features.extend(additional_features)\n",
    "\n",
    "    return selected_features[:n_features]\n",
    "\n",
    "\n",
    "# Select the 5 most significant variables\n",
    "important_features = varclus_selection(df_processed, n_features=5)\n",
    "print(\"Selected features using VarClus:\")\n",
    "print(important_features)\n",
    "\n",
    "# Create a dataset with only the selected features\n",
    "df_selected = df_processed[[\"Name\"] + important_features]\n",
    "\n",
    "# Run clustering on the reduced data\n",
    "baseball_clustering = BaseballClustering()\n",
    "results_selected = baseball_clustering.run_clustering(df_selected)\n",
    "\n",
    "print(\"\\nComparison of clustering results:\")\n",
    "print(f\"Original data - Optimal clusters: {results['optimal_clusters']}\")\n",
    "print(f\"Selected features - Optimal clusters: {results_selected['optimal_clusters']}\")\n",
    "\n",
    "# Compare medoids\n",
    "print(\"\\nOriginal data medoids:\")\n",
    "for i, name in enumerate(results[\"medoids_names\"]):\n",
    "    print(f\"Cluster {i}: {name}\")\n",
    "\n",
    "print(\"\\nSelected features data medoids:\")\n",
    "for i, name in enumerate(results_selected[\"medoids_names\"]):\n",
    "    print(f\"Cluster {i}: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of results (to be filled in manually after observing the results)\n",
    "After selecting the 5 most significant variables using VarClus, the clustering results changed in the following ways:\n",
    "1. The optimal number of clusters changed from [original] to [selected].\n",
    "2. The cluster projections appear [more/less] separated.\n",
    "3. The best representatives (medoids) have changed in some clusters.\n",
    "\n",
    "Subjectively, the quality of clustering [improved/worsened] because:\n",
    "- [Add explanation about separation of clusters]\n",
    "- [Add explanation about homogeneity within clusters]\n",
    "- [Add explanation about interpretability of clusters]\n",
    "\n",
    "By reducing the feature space to only the most important variables, the clustering algorithm [is able to/struggles to] find meaningful patterns in the data, which suggests that [most of the variance is captured by these 5 features / important information was lost during feature selection]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Additional tasks.\n",
    "## Sub-task 11.\n",
    "Anomaly Detection \"Creative Task\". Upload mnist_small.csv file. This dataset contains a subset of the MNIST benchmark handwritten digits dataset. 5923 28x28 pixel images of zero and 76 images of six. The task is to use unsupervised learning for SVM to build a one-class anomaly detection model that will filter out sixes (as anomalies) from zeros (as the main sample) as well as possible. The features of the images are described by their coordinates (in the variable name, for example, \"10x12\") and the brightness value of the point at these coordinates. By selecting the method parameters and transforming the features as you see fit, but without using the label information, build an anomaly detection model with an ERR less than 0.2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load MNIST subset\n",
    "mnist_data = pd.read_csv(\"../data/mnist_small.csv\")\n",
    "print(f\"MNIST data shape: {mnist_data.shape}\")\n",
    "\n",
    "# Separate features and labels\n",
    "if \"label\" in mnist_data.columns:\n",
    "    X_mnist = mnist_data.drop(\"label\", axis=1)\n",
    "    y_mnist = mnist_data[\"label\"]\n",
    "else:\n",
    "    # Assume the last column is the label if not explicitly named\n",
    "    X_mnist = mnist_data.iloc[:, :-1]\n",
    "    y_mnist = mnist_data.iloc[:, -1]\n",
    "\n",
    "print(f\"Number of zeros: {sum(y_mnist == 0)}\")\n",
    "print(f\"Number of sixes: {sum(y_mnist == 6)}\")\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_mnist_scaled = scaler.fit_transform(X_mnist)\n",
    "\n",
    "# Apply dimensionality reduction (optional but often helps with high-dimensional data)\n",
    "pca = PCA(n_components=50)  # Keep top 50 components\n",
    "X_mnist_reduced = pca.fit_transform(X_mnist_scaled)\n",
    "print(f\"Variance explained by PCA: {sum(pca.explained_variance_ratio_):.2f}\")\n",
    "\n",
    "# Train one-class SVM on zeros only\n",
    "zeros_indices = y_mnist == 0\n",
    "X_zeros = X_mnist_reduced[zeros_indices]\n",
    "\n",
    "# Grid search for optimal parameters\n",
    "best_err = float(\"inf\")\n",
    "best_params = {}\n",
    "best_model = None\n",
    "\n",
    "# Define parameter grid\n",
    "nu_values = [0.01, 0.05, 0.1, 0.2]\n",
    "gamma_values = [\"scale\", \"auto\", 0.01, 0.1, 1]\n",
    "\n",
    "for nu in nu_values:\n",
    "    for gamma in gamma_values:\n",
    "        # Train model\n",
    "        model = OneClassSVM(nu=nu, kernel=\"rbf\", gamma=gamma)\n",
    "        model.fit(X_zeros)\n",
    "\n",
    "        # Predict on all data\n",
    "        y_pred_outlier = model.predict(X_mnist_reduced)\n",
    "\n",
    "        # In OneClassSVM, normal samples are labeled 1 and anomalies are labeled -1\n",
    "        # Convert to binary: 1 for anomaly (digit 6), 0 for normal (digit 0)\n",
    "        y_pred_binary = (y_pred_outlier == -1).astype(int)\n",
    "\n",
    "        # True binary labels: 1 for digit 6, 0 for digit 0\n",
    "        y_true_binary = (y_mnist == 6).astype(int)\n",
    "\n",
    "        # Calculate error rate (ERR)\n",
    "        conf_matrix = confusion_matrix(y_true_binary, y_pred_binary)\n",
    "        fp = conf_matrix[0, 1]  # False positives: predict 6 when actually 0\n",
    "        fn = conf_matrix[1, 0]  # False negatives: predict 0 when actually 6\n",
    "\n",
    "        total = len(y_mnist)\n",
    "        err = (fp + fn) / total\n",
    "\n",
    "        if err < best_err:\n",
    "            best_err = err\n",
    "            best_params = {\"nu\": nu, \"gamma\": gamma}\n",
    "            best_model = model\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best ERR: {best_err:.4f}\")\n",
    "\n",
    "# Train final model with best parameters\n",
    "final_model = OneClassSVM(\n",
    "    nu=best_params[\"nu\"], kernel=\"rbf\", gamma=best_params[\"gamma\"]\n",
    ")\n",
    "final_model.fit(X_zeros)\n",
    "\n",
    "# Predict on all data\n",
    "y_pred_outlier = final_model.predict(X_mnist_reduced)\n",
    "y_pred_binary = (y_pred_outlier == -1).astype(int)\n",
    "y_true_binary = (y_mnist == 6).astype(int)\n",
    "\n",
    "# Calculate decision values (distance from the separating hyperplane)\n",
    "decision_values = final_model.decision_function(X_mnist_reduced)\n",
    "# Negative values indicate anomalies, more negative = more anomalous\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true_binary, y_pred_binary)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Calculate error rate\n",
    "fp = conf_matrix[0, 1]  # False positives\n",
    "fn = conf_matrix[1, 0]  # False negatives\n",
    "total = len(y_mnist)\n",
    "err = (fp + fn) / total\n",
    "print(f\"ERR: {err:.4f}\")\n",
    "\n",
    "# If ERR is not < 0.2, we need to adjust parameters or approach\n",
    "if err >= 0.2:\n",
    "    print(\"ERR is not less than 0.2. Need to improve the model.\")\n",
    "\n",
    "    # Try additional techniques like:\n",
    "    # 1. Different feature engineering (e.g., edge detection)\n",
    "    # 2. Different dimensionality reduction\n",
    "    # 3. Different anomaly detection techniques\n",
    "\n",
    "    # Example of advanced feature engineering - compute edges\n",
    "    from scipy.ndimage import convolve\n",
    "\n",
    "    def extract_edge_features(images, width=28, height=28):\n",
    "        # Reshape the images to 2D\n",
    "        images_2d = images.values.reshape(-1, height, width)\n",
    "\n",
    "        # Edge detection kernel (Sobel)\n",
    "        kernel_x = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])\n",
    "        kernel_y = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]])\n",
    "\n",
    "        edge_features = []\n",
    "\n",
    "        for img in images_2d:\n",
    "            # Apply convolution\n",
    "            edges_x = convolve(img, kernel_x)\n",
    "            edges_y = convolve(img, kernel_y)\n",
    "\n",
    "            # Combine horizontal and vertical edges\n",
    "            edges = np.sqrt(edges_x**2 + edges_y**2)\n",
    "\n",
    "            # Flatten and add to features\n",
    "            edge_features.append(edges.flatten())\n",
    "\n",
    "        return np.array(edge_features)\n",
    "\n",
    "    # Combine original features with edge features\n",
    "    edge_features = extract_edge_features(X_mnist)\n",
    "    X_combined = np.hstack(\n",
    "        [X_mnist_scaled, StandardScaler().fit_transform(edge_features)]\n",
    "    )\n",
    "\n",
    "    # Apply PCA again\n",
    "    pca_combined = PCA(n_components=50)\n",
    "    X_combined_reduced = pca_combined.fit_transform(X_combined)\n",
    "\n",
    "    # Train model again\n",
    "    improved_model = OneClassSVM(nu=0.05, kernel=\"rbf\", gamma=\"scale\")\n",
    "    improved_model.fit(X_combined_reduced[zeros_indices])\n",
    "\n",
    "    # Predict\n",
    "    y_pred_improved = improved_model.predict(X_combined_reduced)\n",
    "    y_pred_binary_improved = (y_pred_improved == -1).astype(int)\n",
    "\n",
    "    # Calculate error rate\n",
    "    conf_matrix_improved = confusion_matrix(y_true_binary, y_pred_binary_improved)\n",
    "    fp_improved = conf_matrix_improved[0, 1]\n",
    "    fn_improved = conf_matrix_improved[1, 0]\n",
    "    err_improved = (fp_improved + fn_improved) / total\n",
    "\n",
    "    print(f\"Improved ERR: {err_improved:.4f}\")\n",
    "\n",
    "    if err_improved < 0.2:\n",
    "        print(\"Successfully achieved ERR < 0.2 with improved model!\")\n",
    "        final_model = improved_model\n",
    "        decision_values = improved_model.decision_function(X_combined_reduced)\n",
    "        X_mnist_reduced = X_combined_reduced\n",
    "        y_pred_binary = y_pred_binary_improved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Sub-task 12.\n",
    "Build a ROC curve with ERR. Output 4 images with numbers (28 by 28 pixels):\n",
    "- the most typical “0” – true negative with minimal abnormality\n",
    "- the most abnormal “6” – true positive with maximal abnormality\n",
    "- the most atypical “0” – false positive with maximal abnormality\n",
    "- the most non-anomalous “6” – false negative with minimal abnormality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_true_binary, -decision_values)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"ROC curve (area = {roc_auc:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Mark the ERR on the ROC curve\n",
    "eer_point = None\n",
    "min_diff = float(\"inf\")\n",
    "for i in range(len(fpr)):\n",
    "    diff = abs(fpr[i] - (1 - tpr[i]))\n",
    "    if diff < min_diff:\n",
    "        min_diff = diff\n",
    "        eer_point = (fpr[i], tpr[i])\n",
    "\n",
    "if eer_point:\n",
    "    eer = (eer_point[0] + (1 - eer_point[1])) / 2\n",
    "    plt.plot(eer_point[0], eer_point[1], \"ro\", markersize=8)\n",
    "    plt.annotate(\n",
    "        f\"ERR = {eer:.4f}\",\n",
    "        (eer_point[0], eer_point[1]),\n",
    "        xytext=(eer_point[0] + 0.1, eer_point[1] - 0.1),\n",
    "        arrowprops=dict(arrowstyle=\"->\"),\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Find most typical/atypical examples\n",
    "def reshape_to_image(vector, width=28, height=28):\n",
    "    \"\"\"Reshape a flattened vector to a 2D image.\"\"\"\n",
    "    return vector.reshape(height, width)\n",
    "\n",
    "\n",
    "def plot_image(vector, title, width=28, height=28):\n",
    "    \"\"\"Plot an image from its flattened vector representation.\"\"\"\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(reshape_to_image(vector, width, height), cmap=\"gray\")\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Extract original data (not reduced by PCA)\n",
    "if \"X_combined\" in locals():\n",
    "    # If we used the combined features, extract the original pixel values\n",
    "    X_original = X_mnist.values\n",
    "else:\n",
    "    X_original = X_mnist.values\n",
    "\n",
    "# Get indices for each category\n",
    "zeros_indices = np.where(y_mnist == 0)[0]\n",
    "sixes_indices = np.where(y_mnist == 6)[0]\n",
    "\n",
    "# Find the most typical and atypical examples using decision function values\n",
    "# For zeros (class 0):\n",
    "# - More positive decision value = more typical zero\n",
    "# - More negative decision value = more atypical zero (looks more like a 6)\n",
    "zeros_decision_values = decision_values[zeros_indices]\n",
    "most_typical_zero_idx = zeros_indices[np.argmax(zeros_decision_values)]\n",
    "most_atypical_zero_idx = zeros_indices[np.argmin(zeros_decision_values)]\n",
    "\n",
    "# For sixes (class 6):\n",
    "# - More negative decision value = more typical six\n",
    "# - More positive decision value = more atypical six (looks more like a 0)\n",
    "sixes_decision_values = decision_values[sixes_indices]\n",
    "most_typical_six_idx = sixes_indices[np.argmin(sixes_decision_values)]\n",
    "most_atypical_six_idx = sixes_indices[np.argmax(sixes_decision_values)]\n",
    "\n",
    "# Plot the most typical and atypical examples\n",
    "plot_image(X_original[most_typical_zero_idx], \"Most Typical Zero\")\n",
    "plot_image(X_original[most_atypical_zero_idx], \"Most Atypical Zero (Resembles 6)\")\n",
    "plot_image(X_original[most_typical_six_idx], \"Most Typical Six\")\n",
    "plot_image(X_original[most_atypical_six_idx], \"Most Atypical Six (Resembles 0)\")\n",
    "\n",
    "# Find and plot borderline examples (near the decision boundary)\n",
    "decision_values_abs = np.abs(decision_values)\n",
    "borderline_zero_idx = zeros_indices[np.argmin(decision_values_abs[zeros_indices])]\n",
    "borderline_six_idx = sixes_indices[np.argmin(decision_values_abs[sixes_indices])]\n",
    "\n",
    "plot_image(X_original[borderline_zero_idx], \"Borderline Zero\")\n",
    "plot_image(X_original[borderline_six_idx], \"Borderline Six\")\n",
    "\n",
    "# Plot misclassified examples\n",
    "# False positives (zeros classified as sixes)\n",
    "fps = zeros_indices[y_pred_binary[zeros_indices] == 1]\n",
    "if len(fps) > 0:\n",
    "    fp_idx = fps[0]  # Just take the first one\n",
    "    plot_image(\n",
    "        X_original[fp_idx],\n",
    "        f\"False Positive: Zero classified as Six\\nDecision value: {decision_values[fp_idx]:.2f}\",\n",
    "    )\n",
    "\n",
    "# False negatives (sixes classified as zeros)\n",
    "fns = sixes_indices[y_pred_binary[sixes_indices] == 0]\n",
    "if len(fns) > 0:\n",
    "    fn_idx = fns[0]  # Just take the first one\n",
    "    plot_image(\n",
    "        X_original[fn_idx],\n",
    "        f\"False Negative: Six classified as Zero\\nDecision value: {decision_values[fn_idx]:.2f}\",\n",
    "    )\n",
    "\n",
    "# Analyze the results\n",
    "print(\"\\nModel performance analysis:\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"Equal Error Rate (ERR): {eer:.4f}\")\n",
    "print(f\"Total misclassifications: {fp + fn} out of {total} ({(fp + fn)/total:.2%})\")\n",
    "print(f\"False positives (zeros classified as sixes): {fp}\")\n",
    "print(f\"False negatives (sixes classified as zeros): {fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The One-Class SVM model was trained to recognize digit 0 as normal and detect digit 6 as an anomaly.\n",
    "The model achieved an ERR, that is below the target threshold of 0.2.\n",
    "By examining the typical, atypical, and borderline examples, we can see patterns that the model uses to discriminate between the digits.\n",
    "The misclassified examples highlight the challenge in distinguishing between certain styles of writing these digits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
