{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: \"Finding association rules and revealing hidden structures in data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from networkx import clustering\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "import seaborn as sns\n",
    "\n",
    "from minisom import MiniSom\n",
    "\n",
    "# Normalize the purchase matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 1: Load the Data, Determine Unique Counts\n",
    "\n",
    "Load the file \"TRANSACTION.csv\". \n",
    "It has three columns: \n",
    "- Customer - customer ID, \n",
    "- Product - purchase,\n",
    "- Time - timestamp (not needed for the task).\n",
    "\n",
    "Determine (by writing the corresponding code) how many different values ​​\n",
    "the variables Product and Customer take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"../data/TRANSACTION.csv\")\n",
    "\n",
    "# Determine the number of unique Customers and Products\n",
    "unique_customers = df[\"CUSTOMER\"].nunique()\n",
    "unique_products = df[\"PRODUCT\"].nunique()\n",
    "\n",
    "display(df.head())\n",
    "print(f\"Number of unique customers: {unique_customers}\")\n",
    "print(f\"Number of unique products: {unique_products}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 2: Find Frequent Episodes using FPTree Algorithm\n",
    "\n",
    "Find frequent episodes with a rule size constraint of 4 using the FPTree \n",
    "algorithm and a support threshold of 5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for FPGrowth\n",
    "transactions = df.groupby(\"CUSTOMER\")[\"PRODUCT\"].apply(list).tolist()\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Find frequent itemsets with support threshold of 5% and max_len=4\n",
    "frequent_itemsets: pd.DataFrame = fpgrowth(\n",
    "    df_encoded, min_support=0.05, use_colnames=True, max_len=4\n",
    ")\n",
    "\n",
    "print(frequent_itemsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 3: Find the Largest Frequent Episode containing \"peppers\"\n",
    "\n",
    "Find the largest (most elements) frequent episode containing the product \n",
    "\"peppers\". What support does it have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter itemsets containing 'peppers'\n",
    "peppers_itemsets = frequent_itemsets[\n",
    "    frequent_itemsets[\"itemsets\"].apply(lambda x: \"peppers\" in x)\n",
    "]\n",
    "\n",
    "# Find the itemset with the most elements\n",
    "largest_peppers_itemset = peppers_itemsets.loc[\n",
    "    peppers_itemsets[\"itemsets\"].apply(len).idxmax()\n",
    "]\n",
    "print(\n",
    "    f\"Largest frequent episode containing 'peppers': {largest_peppers_itemset['itemsets']}\"\n",
    ")\n",
    "print(f\"Support: {largest_peppers_itemset['support']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 4: Construct Association Rules with Reliability Threshold of 30%\n",
    "\n",
    "Based on the frequent episodes found, construct association rules with a \n",
    "reliability threshold of 30%. \n",
    "\n",
    "Find the rule with the maximum lift, containing the product \"peppers\" in the \n",
    "left part of the rule. \n",
    "\n",
    "Give it a written verbal interpretation, specify and explain its numerical \n",
    "indicators: support, reliability and lift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.3)\n",
    "\n",
    "# Filter rules with 'peppers' in the left part\n",
    "peppers_rules = rules[rules[\"antecedents\"].apply(lambda x: \"peppers\" in x)]\n",
    "\n",
    "# Find the rule with the maximum lift\n",
    "max_lift_rule = peppers_rules.loc[peppers_rules[\"lift\"].idxmax()]\n",
    "print(\"Rule with maximum lift containing 'peppers' in the antecedent:\")\n",
    "print(f\"{max_lift_rule['antecedents']} -> {max_lift_rule['consequents']}\")\n",
    "print(f\"Support: {max_lift_rule['support']}\")\n",
    "print(f\"Confidence: {max_lift_rule['confidence']}\")\n",
    "print(f\"Lift: {max_lift_rule['lift']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verbal interpretation of the rule:\n",
    "The rule **{'peppers', 'avocado'} -> {'sardines', 'apples'}** means that if a\n",
    "transaction (purchase) includes the products **peppers** and **avocado**, then\n",
    "there is a high probability that **sardines** and **apples** will also be\n",
    "present in the same transaction. This rule describes the associative connection\n",
    "between these products.\n",
    "\n",
    "### Description of numerical indicators:\n",
    "\n",
    "1. **Support: 8.99%**\n",
    "    - This is the proportion of transactions in which all products from the rule are simultaneously present: **peppers, avocado, sardines, apples**.\n",
    "    - In this case, 8.99% of all transactions contain all four products.\n",
    "    - **Conclusion:** This is not very high support, which suggests that this combination of products is relatively rare. However, it can be useful for identifying niche but meaningful associations.\n",
    "\n",
    "2. **Confidence: 71.43%**\n",
    "    - This is the probability that a transaction that contains **peppers** and **avocado** will also contain **sardines** and **apples**.\n",
    "    - In this case, 71.43% of transactions with **peppers** and **avocado** also contain **sardines** and **apples**.\n",
    "    - **Conclusion:** High confidence indicates a strong association between these products. This rule can be considered reliable for prediction.\n",
    "\n",
    "3. **Lift: 5.67**\n",
    "    - The lift shows how much more common the combination of **peppers, avocado, sardines, apples** is compared to if these products were independent of each other.\n",
    "    - A lift value greater than 1 (in this case 5.6746) indicates a positive correlation between the products.\n",
    "    - **Conclusion:** The lift is significantly greater than 1, indicating a strong association between **peppers, avocado** and **sardines, apples**. This means that the presence of **peppers** and **avocado** increases the likelihood of buying **sardines** and **apples** by 5.67 times compared to a random coincidence.\n",
    "\n",
    "### Conclusions based on the values:\n",
    "- **The rule has high reliability (71.43%)**, making it useful for forecasting. - **Lift (5.6746)** indicates a strong association between the products, making this rule meaningful to analyze.\n",
    "- **Support (8.99%)** is not very high, but this may be due to the fact that this combination of products is niche."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 5: Construct Directed Graph from Two-Place Rules\n",
    "\n",
    "Using only two-place rules, construct a directed graph where:\n",
    "- the **vertices are the elements of the rule**, their color (or size) is the \n",
    "item support, \n",
    "- the **arcs are the implications** (oriented in the direction from the \n",
    "condition to the consequence), and the arc weights are the reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rules with single antecedent and single consequent\n",
    "two_place_rules = rules[\n",
    "    (rules[\"antecedents\"].apply(len) == 1) & (rules[\"consequents\"].apply(len) == 1)\n",
    "]\n",
    "\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes with support as size\n",
    "supports = df[\"PRODUCT\"].value_counts(normalize=True)\n",
    "for product, support in supports.items():\n",
    "    G.add_node(product, size=support)\n",
    "\n",
    "# Add edges with confidence as weight\n",
    "for _, row in two_place_rules.iterrows():\n",
    "    antecedent = next(iter(row[\"antecedents\"]))\n",
    "    consequent = next(iter(row[\"consequents\"]))\n",
    "    G.add_edge(antecedent, consequent, weight=row[\"confidence\"])\n",
    "\n",
    "# Draw the graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "pos = nx.spring_layout(G, k=0.5)\n",
    "sizes = [G.nodes[node][\"size\"] * 1000 for node in G.nodes()]\n",
    "weights = [G[u][v][\"weight\"] for u, v in G.edges()]\n",
    "nx.draw_networkx_nodes(G, pos, node_size=sizes, node_color=\"skyblue\")\n",
    "nx.draw_networkx_edges(G, pos, width=weights, edge_color=\"gray\", arrows=True)\n",
    "nx.draw_networkx_labels(\n",
    "    G,\n",
    "    pos,\n",
    "    font_size=8,\n",
    "    verticalalignment=\"top\",\n",
    ")\n",
    "plt.title(\"Directed Graph of Two-Place Association Rules\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 6: Calculate Centrality Measures and Clustering Coefficient\n",
    "\n",
    "For this graph, calculate the centrality measures according to \"Clust. coef\" \n",
    "and find the element with the highest measure, as well as what measure the \n",
    "product \"peppers\" has.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate clustering coefficient\n",
    "clust_coef = clustering(G)\n",
    "\n",
    "# Find the element with the highest clustering coefficient\n",
    "max_clust_element = max(clust_coef, key=clust_coef.get)\n",
    "print(\n",
    "    f\"Element with highest clustering coefficient: {max_clust_element} ({clust_coef[max_clust_element]})\"\n",
    ")\n",
    "\n",
    "# Clustering coefficient of 'peppers'\n",
    "peppers_clust = clust_coef.get(\"peppers\", 0)\n",
    "print(f\"Clustering coefficient of 'peppers': {peppers_clust}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 7: Build Numeric Purchase Matrix\n",
    "\n",
    "Build a numeric matrix with purchase counters in cells, customers in rows, and \n",
    "products in columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchase_matrix = df.pivot_table(\n",
    "    index=\"CUSTOMER\", columns=\"PRODUCT\", aggfunc=\"size\", fill_value=0\n",
    ")\n",
    "display(purchase_matrix.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 8: NMF Linear Projection\n",
    "\n",
    "Using the \"NMF\" method, plot a linear projection of the data set onto a plane \n",
    "(2 components) and color code the transactions containing the product \"peppers\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply NMF\n",
    "nmf = NMF(n_components=2, random_state=42)\n",
    "nmf_features = nmf.fit_transform(purchase_matrix)\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "nmf_df = pd.DataFrame(nmf_features, columns=[\"Component 1\", \"Component 2\"])\n",
    "nmf_df[\"Contains_peppers\"] = purchase_matrix[\"peppers\"] > 0\n",
    "\n",
    "# Plot the projection\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(\n",
    "    x=\"Component 1\",\n",
    "    y=\"Component 2\",\n",
    "    hue=\"Contains_peppers\",\n",
    "    data=nmf_df,\n",
    "    palette=[\"blue\", \"red\"],\n",
    ")\n",
    "plt.title(\"NMF Linear Projection with 'peppers' Highlighted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 9: SOM Nonlinear Projection\n",
    "\n",
    "Using the \"SOM\" method, construct a nonlinear projection of the data set onto a \n",
    "plane; color-code the transactions containing the product \"peppers\". \n",
    "Parameters not specified in the task (for example, the lattice size for the SOM\n",
    "or the number of layers in the autoencoder) can be chosen at your discretion to\n",
    "obtain the most convenient visualization. Provide a written comment on how, from\n",
    " your point of view, a nonlinear projection is better or worse for your example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'scaled_matrix' is your preprocessed data matrix\n",
    "scaler = MinMaxScaler()\n",
    "scaled_matrix = scaler.fit_transform(purchase_matrix)\n",
    "\n",
    "# Initialize the SOM\n",
    "som = MiniSom(\n",
    "    x=10, y=10, input_len=scaled_matrix.shape[1], sigma=1.0, learning_rate=0.5\n",
    ")\n",
    "som.random_weights_init(scaled_matrix)\n",
    "som.train_random(scaled_matrix, 1000)\n",
    "\n",
    "# Mapping vectors to their winning neurons manually\n",
    "mapped = np.array([som.winner(x) for x in scaled_matrix])\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "som_df = pd.DataFrame(mapped, columns=[\"x\", \"y\"])\n",
    "\n",
    "# Optional: Visualize the SOM\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.pcolor(som.distance_map().T, cmap=\"Blues\")  # Distance map as background\n",
    "plt.colorbar()\n",
    "\n",
    "plt.title(\"Self-Organizing Map\")\n",
    "plt.show()\n",
    "\n",
    "# Comment on SOM projection\n",
    "print(\n",
    "    \"\"\"\n",
    "    The SOM nonlinear projection allows capturing complex relationships\n",
    "    between products, potentially revealing clusters that linear methods like\n",
    "    NMF might miss. However, it may require careful parameter tuning to \n",
    "    achieve meaningful visualizations.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 10: Select 6 Independent Variables using Stepwise Selection\n",
    "\n",
    "From the original matrix (from point 7), according to your option, select 6 \n",
    "independent variables using any of the methods: VarClus, Glasso or step-by-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Selecting variables based on predicting 'peppers'\n",
    "# Create target variable\n",
    "purchase_matrix[\"peppers_flag\"] = purchase_matrix[\"peppers\"] > 0\n",
    "\n",
    "# Features and target\n",
    "X = purchase_matrix.drop([\"peppers\", \"peppers_flag\"], axis=1)\n",
    "y = purchase_matrix[\"peppers_flag\"]\n",
    "\n",
    "# Initialize model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Initialize RFE\n",
    "rfe = RFE(model, n_features_to_select=6)\n",
    "fit = rfe.fit(X, y)\n",
    "\n",
    "# Selected features\n",
    "selected_features = X.columns[fit.support_]\n",
    "print(\"Selected independent variables:\", list(selected_features))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
